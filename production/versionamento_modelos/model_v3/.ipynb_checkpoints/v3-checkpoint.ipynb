{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "721f6ce1",
   "metadata": {},
   "source": [
    "# Versão 3 dos testes realizados para o modelo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b6c1875-61e2-49a6-acfc-add704f77697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, re, io\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6da7fbaf-2088-489a-9852-8928c95fc734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categoria</th>\n",
       "      <th>Texto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Turismo</td>\n",
       "      <td>Turismo: esse é o Destino. &lt;p&gt;&lt;strong&gt;Objetivo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Desenvolvimento Agrário e Agricultura Familiar</td>\n",
       "      <td>Agricultura Familiar e Agroecologia. &lt;p&gt;&lt;stron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Agricultura e Pecuária</td>\n",
       "      <td>Agropecuária Sustentável. &lt;p&gt;Objetivo: Contrib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Saúde</td>\n",
       "      <td>Atenção Primária à Saúde. &lt;p&gt;Fortalecer a Aten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Saúde</td>\n",
       "      <td>Atenção Especializada à Saúde. &lt;p&gt;Ampliar o ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8431</th>\n",
       "      <td>Saúde</td>\n",
       "      <td>Microchipagem de animais de companhia. Microch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8432</th>\n",
       "      <td>Secretaria Geral da Presidência da República</td>\n",
       "      <td>Direito a atendimento eficiente, é um direito...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8433</th>\n",
       "      <td>Previdência Social</td>\n",
       "      <td>Rever Reforma da Previdência. Rever a reforma ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8434</th>\n",
       "      <td>Meio Ambiente e Mudança do Clima</td>\n",
       "      <td>Preservação da Serra do Espinhaço. A Serra do ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8435</th>\n",
       "      <td>Saúde</td>\n",
       "      <td>Piso salarial pra todos.. Pra nós todos ter o ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8436 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Categoria  \\\n",
       "0                                            Turismo   \n",
       "1     Desenvolvimento Agrário e Agricultura Familiar   \n",
       "2                             Agricultura e Pecuária   \n",
       "3                                              Saúde   \n",
       "4                                              Saúde   \n",
       "...                                              ...   \n",
       "8431                                           Saúde   \n",
       "8432    Secretaria Geral da Presidência da República   \n",
       "8433                              Previdência Social   \n",
       "8434                Meio Ambiente e Mudança do Clima   \n",
       "8435                                           Saúde   \n",
       "\n",
       "                                                  Texto  \n",
       "0     Turismo: esse é o Destino. <p><strong>Objetivo...  \n",
       "1     Agricultura Familiar e Agroecologia. <p><stron...  \n",
       "2     Agropecuária Sustentável. <p>Objetivo: Contrib...  \n",
       "3     Atenção Primária à Saúde. <p>Fortalecer a Aten...  \n",
       "4     Atenção Especializada à Saúde. <p>Ampliar o ac...  \n",
       "...                                                 ...  \n",
       "8431  Microchipagem de animais de companhia. Microch...  \n",
       "8432   Direito a atendimento eficiente, é um direito...  \n",
       "8433  Rever Reforma da Previdência. Rever a reforma ...  \n",
       "8434  Preservação da Serra do Espinhaço. A Serra do ...  \n",
       "8435  Piso salarial pra todos.. Pra nós todos ter o ...  \n",
       "\n",
       "[8436 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lê as propostas com o Pandas\n",
    "propostas = pd.read_csv(\"/home/joaopedro/07_ProcessamentoLinguagemNatural/production/data_extraction/propostas.csv\")\n",
    "\n",
    "propostas  # Impressão do DataFrame de propostas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aeabc13-26e3-4df6-aaae-fb534093eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expressões regulares para remoção de poluição de dados nos textos \n",
    "# REGX_URL = r\"https?://[A-Za-z0-9./\\-]+\" # Regex for URLs\n",
    "REGX_HTML = r\"<[^<]+?>\" # Regex for HTML tags\n",
    "REGX_ENDING = r'Órgão Responsável:.+' # Regex for the part of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ee71bc1-5fb2-4b66-a8da-a1b427dc1f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categoria</th>\n",
       "      <th>Texto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Turismo</td>\n",
       "      <td>Turismo: esse é o Destino. Objetivo: Posiciona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Desenvolvimento Agrário e Agricultura Familiar</td>\n",
       "      <td>Agricultura Familiar e Agroecologia. Objetivo:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Agricultura e Pecuária</td>\n",
       "      <td>Agropecuária Sustentável. Objetivo: Contribuir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Saúde</td>\n",
       "      <td>Atenção Primária à Saúde. Fortalecer a Atenção...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Saúde</td>\n",
       "      <td>Atenção Especializada à Saúde. Ampliar o acess...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Categoria  \\\n",
       "0                                         Turismo   \n",
       "1  Desenvolvimento Agrário e Agricultura Familiar   \n",
       "2                          Agricultura e Pecuária   \n",
       "3                                           Saúde   \n",
       "4                                           Saúde   \n",
       "\n",
       "                                               Texto  \n",
       "0  Turismo: esse é o Destino. Objetivo: Posiciona...  \n",
       "1  Agricultura Familiar e Agroecologia. Objetivo:...  \n",
       "2  Agropecuária Sustentável. Objetivo: Contribuir...  \n",
       "3  Atenção Primária à Saúde. Fortalecer a Atenção...  \n",
       "4  Atenção Especializada à Saúde. Ampliar o acess...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Função para pré-processamento\n",
    "def preprocessing(text):\n",
    "  # text = text.lower()\n",
    "\n",
    "  text = re.sub(REGX_HTML, '', text)  # Removendo tags HTML\n",
    "  # text = re.sub(REGX_URL, '', text) # Revomendo URLs\n",
    "  text = re.sub(REGX_ENDING, '', text)\n",
    "\n",
    "  # tokens = [t.lemma_ for t in nlp(text) if t not in STOP_WORDS and not t.is_punct]\n",
    "\n",
    "  return text\n",
    "\n",
    "#  Ajustando textos da coluna Corpo\n",
    "propostas['Texto'] = propostas['Texto'].apply(preprocessing)\n",
    "propostas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f286d88",
   "metadata": {},
   "source": [
    "## Adcionando dados abertos do Ministerio de  Relações Exteriores  para a categora de  Relações Exteriores "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1065f5",
   "metadata": {},
   "source": [
    "# Dados extraidos do concordia MRE-Gov (https://dados.gov.br/dados/organizacoes/visualizar/ministerio-das-relacoes-exteriores-mre) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d61b2906",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../production/data_extraction/MRE.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchardet\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Detecta a codificação do arquivo CSV\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../production/data_extraction/MRE.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      5\u001b[0m     result \u001b[38;5;241m=\u001b[39m chardet\u001b[38;5;241m.\u001b[39mdetect(f\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m      6\u001b[0m encoding \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../production/data_extraction/MRE.csv'"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "# Detecta a codificação do arquivo CSV\n",
    "with open('/home/joaopedro/07_ProcessamentoLinguagemNatural/production/data_extraction/MRE.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "encoding = result['encoding']\n",
    "\n",
    "# Lê o CSV usando a codificação detectada\n",
    "relacoes_ext = pd.read_csv('/home/joaopedro/07_ProcessamentoLinguagemNatural/production/data_extraction/MRE.csv', sep=';', encoding=encoding)\n",
    "\n",
    "# retira colunas \n",
    "colunas_retiradas = ['Unnamed: 1', 'Unnamed: 2','Unnamed: 3','Unnamed: 4','Unnamed: 6','Unnamed: 7','Unnamed: 8','Unnamed: 9','Unnamed: 10']\n",
    "relacoes_ext = relacoes_ext.drop(index=0,columns=colunas_retiradas)\n",
    "\n",
    "# Renomeia colunas e retira primeira linha\n",
    "\n",
    "relacoes_ext.columns = ['Categoria', 'Texto']\n",
    "\n",
    "# Combina os textos das colunas 1 e 2 na coluna 2\n",
    "relacoes_ext['Texto'] = relacoes_ext['Categoria'] + ' ' + relacoes_ext['Texto']\n",
    "\n",
    "# Renomeie todas as linhas da coluna 'Categoria' para 'Relações Exteriores'\n",
    "relacoes_ext['Categoria'] = 'Relações Exteriores'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ce2b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removendo as tags html da cooluna texto\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Função para limpar os dados de tags HTML e quebras de linha\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Verifica se é uma string\n",
    "        # Remove as tags HTML com BeautifulSoup\n",
    "        cleaned = BeautifulSoup(text, 'html.parser').get_text()\n",
    "        # Remove as quebras de linha com expressões regulares\n",
    "        cleaned = re.sub(r'\\n', ' ', cleaned)\n",
    "        return cleaned.strip()\n",
    "    return text  # Retorna o valor original se não for uma string\n",
    "\n",
    "# Aplica a função de limpeza aos dados da coluna 'Texto'\n",
    "relacoes_ext['Texto'] = relacoes_ext['Texto'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1987602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "relacoes_ext.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9486ebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use o método concat para adicionar os dados do DataFrame relacoes_ext ao DataFrame propostas\n",
    "frames = [propostas,relacoes_ext]\n",
    "propostas = pd.concat(frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7a1fc2",
   "metadata": {},
   "source": [
    "# Realizando a primeira separacao em dados de treino e teste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0638d5c-3f3b-4ced-b448-08557bc6d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "treino, teste, classe_treino, classe_teste = train_test_split(propostas.Texto,\n",
    "                                                              propostas.Categoria,\n",
    "                                                              random_state = 42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67505c2f-8f07-4e38-81d1-32f6d378d2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcb77ca-00dd-4c23-a5bf-b20449642c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "classe_treino\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d09075-1955-41c4-b6f5-d96d5eed5541",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(propostas.Categoria.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1594b8-dfcd-4c08-b795-ce310818924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "propostas['Texto'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baed5a8-44bc-46fe-8a85-012090438d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando índices para as categorias\n",
    "cats = propostas['Categoria'].unique() # Pegando cada categoria única\n",
    "cats = dict(enumerate(cats, 0)) # Convertendo para dict (com índices enumerados)\n",
    "cats = {v:k for k,v in cats.items()}  # Trocando chaves e valores\n",
    "\n",
    "propostas['id_cats'] = propostas['Categoria'].map(cats) # Inserindo coluna de índices das cats\n",
    "propostas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049f36ad-30a5-494d-8172-eeb210eef3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remova valores não processáveis da coluna 'Texto'\n",
    "propostas = propostas[propostas['Texto'].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "# Inicialize o modelo de regressão logística\n",
    "regressao_logistica = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "\n",
    "def classificar_texto(texto, coluna_texto, coluna_classificacao):\n",
    "    vetorizar = CountVectorizer(lowercase=False, max_features=50)\n",
    "    bag_of_words = vetorizar.fit_transform(texto[coluna_texto])\n",
    "    treino, teste, classe_treino, classe_teste = train_test_split(bag_of_words,\n",
    "                                                                  texto[coluna_classificacao],\n",
    "                                                                  random_state=42)\n",
    "    regressao_logistica.fit(treino, classe_treino)\n",
    "    return regressao_logistica.score(teste, classe_teste)\n",
    "\n",
    "acuracia_bruta = classificar_texto(propostas, \"Texto\", \"id_cats\")\n",
    "\n",
    "print(\"Acurácia:\", acuracia_bruta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b6ef47-ca58-4db6-9454-7d63d74fdecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "todas_palavras = ' '.join([texto for texto in propostas.Texto])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca3b63c-918d-425a-a9d7-11defc3a8844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "\n",
    "# realiza a tokenização do texto\n",
    "token_espaco = tokenize.WhitespaceTokenizer()\n",
    "token_frase = token_espaco.tokenize(todas_palavras)\n",
    "\n",
    "frequencia = nltk.FreqDist(token_frase)\n",
    "df_frequencia = pd.DataFrame({\"Palavra\": list(frequencia.keys()),\n",
    "                                   \"Frequência\": list(frequencia.values())})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445bb21b-e1f8-446e-a489-6b892d1fd452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imprime um data frame com as palavras ordenadas pelas q tem a maior frequencia\n",
    "df_frequencia.nlargest(columns = \"Frequência\", n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8b45d7-cf72-468a-868d-8510a7700981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def pareto(texto, coluna_texto, quantidade):\n",
    "    todas_palavras = ' '.join([texto for texto in texto[coluna_texto]])\n",
    "    token_frase = token_espaco.tokenize(todas_palavras)\n",
    "    frequencia = nltk.FreqDist(token_frase)\n",
    "    df_frequencia = pd.DataFrame({\"Palavra\": list(frequencia.keys()),\n",
    "                                   \"Frequência\": list(frequencia.values())})\n",
    "    df_frequencia = df_frequencia.nlargest(columns = \"Frequência\", n = quantidade)\n",
    "    plt.figure(figsize=(12,8))\n",
    "    ax = sns.barplot(data = df_frequencia, x = \"Palavra\", y = \"Frequência\", color = 'gray')\n",
    "    ax.set(ylabel = \"Contagem\")\n",
    "    plt.show()\n",
    "\n",
    "pareto(propostas, \"Texto\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964cb873-df86-4835-adfe-0a1f6df4e0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras_irrelevantes = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "\n",
    "frase_processada = list()\n",
    "for opiniao in propostas.Texto:\n",
    "    nova_frase = list()\n",
    "    palavras_texto = token_espaco.tokenize(opiniao)\n",
    "    for palavra in palavras_texto:\n",
    "        if palavra not in palavras_irrelevantes:\n",
    "            nova_frase.append(palavra)\n",
    "    frase_processada.append(' '.join(nova_frase))\n",
    "\n",
    "propostas[\"tratamento_1\"] = frase_processada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbe6f0b-6188-4f65-8e29-9cbe770746dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "propostas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6fb82a-7643-4d59-8b5e-cb968e6aaadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "acuracia_tratamento_1 = classificar_texto(propostas, \"tratamento_1\", \"id_cats\")\n",
    "print(acuracia_tratamento_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9cefac-4761-432f-acf0-524a939e6931",
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto(propostas,\"tratamento_1\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9fee95-0834-40a6-a7b9-0467538cbeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "token_pontuacao = tokenize.WordPunctTokenizer()\n",
    "\n",
    "pontuacao = list()\n",
    "for ponto in punctuation:\n",
    "    pontuacao.append(ponto)\n",
    "\n",
    "pontuacao_stopwords = pontuacao + palavras_irrelevantes\n",
    "\n",
    "frase_processada = list()\n",
    "for opiniao in propostas[\"tratamento_1\"]:\n",
    "    nova_frase = list()\n",
    "    palavras_texto = token_pontuacao.tokenize(opiniao)\n",
    "    for palavra in palavras_texto:\n",
    "        if palavra not in pontuacao_stopwords:\n",
    "            nova_frase.append(palavra)\n",
    "    frase_processada.append(' '.join(nova_frase))\n",
    "\n",
    "propostas[\"tratamento_2\"] = frase_processada\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e04939d-aec7-4dc3-b37b-3ba830ec23d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "propostas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6128d1bf-764f-48c4-a767-503ae17a49f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "acuracia_tratamento2 = classificar_texto(propostas, \"tratamento_2\", \"id_cats\")\n",
    "print(acuracia_tratamento2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79626c42-361e-4fb5-ba55-9bcf8a5b7c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "propostas[\"tratamento_1\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe90e0e-b0dc-47fc-8ee0-9ef9bb17c582",
   "metadata": {},
   "outputs": [],
   "source": [
    "propostas[\"tratamento_2\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f4e361-bcab-4b5d-93fe-c22bf4f75bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto(propostas,\"tratamento_2\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e6c5ea-fad0-4df2-9d72-fceee912c3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode \n",
    "\n",
    "sem_acentos = [unidecode.unidecode(texto) for texto in propostas[\"tratamento_2\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa163322-956e-4561-b78e-236f899fddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_acentos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43026e3e-cde5-43d2-b984-72a56f925dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_sem_acento = [unidecode.unidecode(texto) for texto in pontuacao_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c8ffb2-bfc4-49c9-be79-89285794f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_sem_acento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba35fd2-5049-42d1-bd47-4f4590e814c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "propostas[\"tratamento_3\"] = sem_acentos\n",
    "\n",
    "frase_processada = list()\n",
    "for opiniao in propostas[\"tratamento_3\"]:\n",
    "    nova_frase = list()\n",
    "    palavras_texto = token_pontuacao.tokenize(opiniao)\n",
    "    for palavra in palavras_texto:\n",
    "        if palavra not in pontuacao_stopwords:\n",
    "            nova_frase.append(palavra)\n",
    "    frase_processada.append(' '.join(nova_frase))\n",
    "\n",
    "propostas[\"tratamento_3\"] = frase_processada\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f111e6-9fc0-4f77-9cf7-b7b2b9111820",
   "metadata": {},
   "outputs": [],
   "source": [
    "propostas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db02ebfc-c088-4a5f-9636-f11b50ab4a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "acuracia_tratamento3 = classificar_texto(propostas, \"tratamento_3\", \"id_cats\")\n",
    "print(acuracia_tratamento3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea54341-3162-48f9-88d6-e573a9b919b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deixa todas as letras minusculas\n",
    "frase_processada = list()\n",
    "for opiniao in propostas[\"tratamento_3\"]:\n",
    "    nova_frase = list()\n",
    "    opiniao = opiniao.lower()\n",
    "    palavras_texto = token_pontuacao.tokenize(opiniao)\n",
    "    for palavra in palavras_texto:\n",
    "        if palavra not in stopwords_sem_acento:\n",
    "            nova_frase.append(palavra)\n",
    "    frase_processada.append(' '.join(nova_frase))\n",
    "\n",
    "propostas[\"tratamento_4\"] = frase_processada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa9ba93-525c-44b0-9093-1f783c8836b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "propostas[\"tratamento_3\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c3489b-3317-4a90-a45b-7cd8e4adb0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "propostas[\"tratamento_4\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d9a9e9-0fca-4faf-8fe5-2048bc126dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "acuracia_tratamento4 = classificar_texto(propostas, \"tratamento_4\", \"id_cats\")\n",
    "print(acuracia_tratamento4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be3598d-d26b-4249-923e-52959bd6e23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto(propostas, \"tratamento_4\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda8bdab-39e2-4633-9f97-027cd2b74a11",
   "metadata": {},
   "source": [
    "# Teste com a Stemização\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d19829-899a-4dad-a86a-de9619dfa90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('rslp')\n",
    "# faz a stemetizacao\n",
    "stemmer = nltk.RSLPStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e67127-e0e1-4d30-b49b-c82946d295ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "frase_processada = list()\n",
    "for opiniao in propostas[\"tratamento_4\"]:\n",
    "    nova_frase = list()\n",
    "    palavras_texto = token_pontuacao.tokenize(opiniao)\n",
    "    for palavra in palavras_texto:\n",
    "        if palavra not in stopwords_sem_acento:\n",
    "            nova_frase.append(stemmer.stem(palavra))\n",
    "    frase_processada.append(' '.join(nova_frase))\n",
    "\n",
    "propostas[\"tratamento_5\"] = frase_processada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29f826b-b425-4dbe-8fed-a31224f276cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "acuracia_tratamento5 = classificar_texto(propostas, \"tratamento_5\", \"id_cats\")\n",
    "print(acuracia_tratamento5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0a947a-a3b3-46d1-aeb1-52cfd2c4a4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto(propostas, \"tratamento_5\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbf0f6f-d33a-4543-b5d6-574f80d59791",
   "metadata": {},
   "source": [
    "# Testando a utilização do TF-IDF no dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6c0bc1-23ca-4ed3-803f-a093d290d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(lowercase=False, max_features=50)\n",
    "\n",
    "tfidf_tratados = tfidf.fit_transform(propostas[\"tratamento_5\"])\n",
    "treino, teste, classe_treino, classe_teste = train_test_split(tfidf_tratados,\n",
    "                                                              propostas[\"id_cats\"],\n",
    "                                                              random_state = 42)\n",
    "regressao_logistica.fit(treino, classe_treino)\n",
    "acuracia_tfidf_tratados = regressao_logistica.score(teste, classe_teste)\n",
    "\n",
    "print(acuracia_tfidf_tratados)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daf2001-4675-4715-bfdd-b69c41854048",
   "metadata": {},
   "source": [
    "# Testando a utilizacao do TF-IDF juntamente com o ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ab7832-7a48-4558-aea5-d145d1a3b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "#acuracia do tfidf utilizando os ngrams\n",
    "tfidf = TfidfVectorizer(lowercase=False, ngram_range = (1,2))\n",
    "vetor_tfidf = tfidf.fit_transform(propostas[\"tratamento_5\"])\n",
    "treino, teste, classe_treino, classe_teste = train_test_split(vetor_tfidf,\n",
    "                                                              propostas[\"id_cats\"],\n",
    "                                                              random_state = 42)\n",
    "regressao_logistica.fit(treino, classe_treino)\n",
    "acuracia_tfidf_ngrams = regressao_logistica.score(teste, classe_teste)\n",
    "print(acuracia_tfidf_ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1a74be-d54d-48d1-baa0-04659bd2f6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#palavras que o modelo aprendeu da categoria saude (id_cats 3)\n",
    "pesos = pd.DataFrame(\n",
    "    regressao_logistica.coef_[3].T,\n",
    "    index = tfidf.get_feature_names_out()\n",
    ")\n",
    "\n",
    "pesos.nlargest(50,0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d837fb0-3b84-4d20-ada7-9812d4fa18a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dados numéricos\n",
    "dados = [acuracia_bruta * 100, acuracia_tratamento_1 * 100, acuracia_tratamento2 * 100,\n",
    "         acuracia_tratamento3 * 100, acuracia_tratamento4 * 100, acuracia_tratamento5 * 100,\n",
    "         acuracia_tfidf_ngrams * 100]\n",
    "etiquetas = ['bruta', '1', '2', '3', '4', '5', 'stem ngram']\n",
    "\n",
    "# Criar o gráfico de barras\n",
    "plt.bar(etiquetas, dados, color='blue')\n",
    "\n",
    "# Adicionar título e rótulos dos eixos\n",
    "plt.title('Acuracia tratamentos')\n",
    "plt.xlabel('Tratamentos')\n",
    "plt.ylabel('Acuracia')\n",
    "\n",
    "# Adicionar os valores acima de cada barra\n",
    "for i, valor in enumerate(dados):\n",
    "    plt.text(i, valor + 1, f'{valor:.2f}%', ha='center')\n",
    "\n",
    "# Exibir o gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a513c40-19da-4fed-a9d2-6e2ab1c78a33",
   "metadata": {},
   "source": [
    "# Testando com o SVM utilizando o tratamento 4 ou seja sem a stemizacao\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fefc009-49bb-4441-b284-9d9e5b401741",
   "metadata": {},
   "outputs": [],
   "source": [
    "propostas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd32561-da5d-4046-ba57-8948ce15ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938e4a05-2f45-4555-a6b2-a1589e77fd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Representações vetoriais do texto\n",
    "propostas['Vector'] = propostas['tratamento_4'].apply(lambda text: nlp(text).vector)\n",
    "propostas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724e943e-a500-46e8-af7d-579468d3f690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import svm, model_selection\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404fb30f-42f7-43c0-82ac-a33580aee82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separação dos dados para treinamento e teste\n",
    "treino, teste, classe_treino, classe_teste = model_selection.train_test_split(\n",
    "    propostas.Vector.values,\n",
    "    propostas.Categoria,\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "# Transformação: conversão do tipo dos dados de entrada (X) para interpretação\n",
    "treino_2d = np.stack(treino)\n",
    "teste_2d = np.stack(teste)\n",
    "\n",
    "# Transformação: escalonamento dos dados numéricos (normalizados entre 0 e 1)\n",
    "# Para tratamento de casos outliers\n",
    "scaler = MinMaxScaler()\n",
    "scaled_train_embed = scaler.fit_transform(treino_2d)\n",
    "scaled_test_embed = scaler.transform(teste_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088495b7-9d96-43be-9c78-5e3ea1170f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Treinamento\n",
    "clf = svm.SVC() # Instanciando modelo\n",
    "clf.fit(scaled_train_embed, classe_treino)  # Realização do treinamento passando os dados\n",
    "\n",
    "classe_pred = clf.predict(scaled_test_embed) # Predição com os dados de teste\n",
    "\n",
    "# Impressão do relatório com as métricas do modelo (para cada cat.)\n",
    "print(classification_report(classe_teste, classe_pred))  \n",
    "\n",
    "predictions_SVM = clf.predict(scaled_test_embed)\n",
    "print(\"SVM Accuracy Score -> \", accuracy_score(predictions_SVM, classe_teste)*100)  # Impressão da precisão geral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88578e4a-79f1-4187-a5e3-94e601ab68d7",
   "metadata": {},
   "source": [
    "# Teste da regressao logistica com a lematizacao usando o spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f182fd17-cb53-41e6-bcb3-47688ea356c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "propostas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24fa687-a481-4bc2-b432-86841c9e9963",
   "metadata": {},
   "outputs": [],
   "source": [
    "frase_processada = list()\n",
    "for opiniao in propostas[\"tratamento_4\"]:\n",
    "    nova_frase = list()\n",
    "    doc = nlp(opiniao)\n",
    "    for token in doc:\n",
    "        nova_frase.append(token.lemma_)\n",
    "    frase_processada.append(' '.join(nova_frase))\n",
    "\n",
    "propostas[\"tratamento_6\"] = frase_processada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b41c19-3a39-40e2-9c26-58b630ae4379",
   "metadata": {},
   "outputs": [],
   "source": [
    "propostas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04001813-e34d-4554-95b6-9e85d05422c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "#acuracia do tfidf utilizando os ngrams\n",
    "tfidf = TfidfVectorizer(lowercase=False, ngram_range = (1,2))\n",
    "vetor_tfidf = tfidf.fit_transform(propostas[\"tratamento_6\"])\n",
    "treino, teste, classe_treino, classe_teste = train_test_split(vetor_tfidf,\n",
    "                                                              propostas[\"id_cats\"],\n",
    "                                                              random_state = 42)\n",
    "regressao_logistica.fit(treino, classe_treino)\n",
    "acuracia_lemma = regressao_logistica.score(teste, classe_teste)\n",
    "print(acuracia_lemma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d8cf63-d77b-4623-8c4d-03e8e5b09b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dados numéricos\n",
    "dados = [acuracia_bruta * 100, acuracia_tratamento_1 * 100, acuracia_tratamento2 * 100,\n",
    "         acuracia_tratamento3 * 100, acuracia_tratamento4 * 100, acuracia_tratamento5 * 100,\n",
    "         acuracia_tfidf_ngrams * 100,acuracia_lemma * 100]\n",
    "etiquetas = ['bruta', '1', '2', '3', '4', '5', 'stem','lemma']\n",
    " \n",
    "# Criar o gráfico de barras\n",
    "plt.bar(etiquetas, dados, color='blue')\n",
    "\n",
    "# Adicionar título e rótulos dos eixos\n",
    "plt.title('Acuracia tratamentos')\n",
    "plt.xlabel('Tratamentos')\n",
    "plt.ylabel('Acuracia')\n",
    "\n",
    "# Adicionar os valores acima de cada barra\n",
    "for i, valor in enumerate(dados):\n",
    "    plt.text(i, valor + 1, f'{valor:.2f}%', ha='center')\n",
    "\n",
    "# Exibir o gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f9c6e6-9e6b-46d9-9992-9e14017b74ab",
   "metadata": {},
   "source": [
    "# Aplicando o TF-IDF com Ngrams nos dados sem stemizacao e lematizacao no SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a804045-f247-4161-90f0-2b56211e951a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Dividir dados em treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(propostas['tratamento_4'],propostas[\"Categoria\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Criar vetorizador TF-IDF com n-grams\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # Aqui usamos bigrams\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Treinar SVM\n",
    "svm_model = SVC(kernel='linear')  # Pode usar outros kernels também\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Prever e avaliar\n",
    "y_pred = svm_model.predict(X_test_tfidf)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23102735-e3ce-4854-987b-b8a29ee59eed",
   "metadata": {},
   "source": [
    "# Aplicando o TF-IDF com Ngrams nos dados com lematizacao no SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a7c278-5baf-4356-953e-fc6d71cbf074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Dividir dados em treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(propostas['tratamento_6'],propostas[\"Categoria\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Criar vetorizador TF-IDF com n-grams\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # Aqui usamos bigrams\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Treinar SVM\n",
    "svm_model = SVC(kernel='linear')  # Pode usar outros kernels também\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Prever e avaliar\n",
    "y_pred = svm_model.predict(X_test_tfidf)\n",
    "report = classification_report(y_test, y_pred,output_dict=True)\n",
    "\n",
    "# Salvando relatório de treinamento em um arquivo JSON\n",
    "json_obj = json.dumps(report, indent=4)\n",
    "save_file = open('../model_v3/ttraining_results.json', 'w').write(json_obj)\n",
    "\n",
    "\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
