{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# model building imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "from keras.layers import Conv1D, SimpleRNN, Bidirectional, MaxPooling1D, GlobalMaxPool1D, LSTM, GRU\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import L1L2\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffpost.com/entry/covid-boosters-...</td>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Health experts said it is too early to predict...</td>\n",
       "      <td>Carla K. Johnson, AP</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffpost.com/entry/american-airlin...</td>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>He was subdued by passengers and crew when he ...</td>\n",
       "      <td>Mary Papenfuss</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-tweets...</td>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "      <td>Elyse Wanshel</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-parent...</td>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "      <td>Caroline Bologna</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffpost.com/entry/amy-cooper-lose...</td>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
       "      <td>Nina Golgowski</td>\n",
       "      <td>2022-09-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  https://www.huffpost.com/entry/covid-boosters-...   \n",
       "1  https://www.huffpost.com/entry/american-airlin...   \n",
       "2  https://www.huffpost.com/entry/funniest-tweets...   \n",
       "3  https://www.huffpost.com/entry/funniest-parent...   \n",
       "4  https://www.huffpost.com/entry/amy-cooper-lose...   \n",
       "\n",
       "                                            headline   category  \\\n",
       "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
       "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
       "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
       "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
       "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
       "\n",
       "                                   short_description               authors  \\\n",
       "0  Health experts said it is too early to predict...  Carla K. Johnson, AP   \n",
       "1  He was subdued by passengers and crew when he ...        Mary Papenfuss   \n",
       "2  \"Until you have a dog you don't understand wha...         Elyse Wanshel   \n",
       "3  \"Accidentally put grown-up toothpaste on my to...      Caroline Bologna   \n",
       "4  Amy Cooper accused investment firm Franklin Te...        Nina Golgowski   \n",
       "\n",
       "        date  \n",
       "0 2022-09-23  \n",
       "1 2022-09-23  \n",
       "2 2022-09-23  \n",
       "3 2022-09-23  \n",
       "4 2022-09-22  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en = spacy.load('en_core_web_md')\n",
    "STOPWORDS = en.Defaults.stop_words\n",
    "\n",
    "df = pd.read_json('../model/News_Category_Dataset_v3.json', lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Health experts said it is too early to predict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>He was subdued by passengers and crew when he ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline   category  \\\n",
       "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
       "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
       "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
       "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
       "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
       "\n",
       "                                   short_description  \n",
       "0  Health experts said it is too early to predict...  \n",
       "1  He was subdued by passengers and crew when he ...  \n",
       "2  \"Until you have a dog you don't understand wha...  \n",
       "3  \"Accidentally put grown-up toothpaste on my to...  \n",
       "4  Amy Cooper accused investment firm Franklin Te...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = df.drop(columns=['authors','link','date'])\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>length_of_news</th>\n",
       "      <th>len_news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COMEDY</td>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PARENTING</td>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    category                                     length_of_news  len_news\n",
       "0  U.S. NEWS  Over 4 Million Americans Roll Up Sleeves For O...       230\n",
       "1  U.S. NEWS  American Airlines Flyer Charged, Banned For Li...       248\n",
       "2     COMEDY  23 Of The Funniest Tweets About Cats And Dogs ...       133\n",
       "3  PARENTING  The Funniest Tweets From Parents This Week (Se...       215\n",
       "4  U.S. NEWS  Woman Who Called Cops On Black Bird-Watcher Lo...       233"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = new_df.copy()\n",
    "final_df['length_of_news'] = final_df['headline'] + final_df['short_description']\n",
    "final_df.drop(['headline','short_description'], inplace=True, axis=1)\n",
    "final_df['len_news'] = final_df['length_of_news'].map(lambda x: len(x))\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datacleaning(text):\n",
    "    whitespace = re.compile(r\"\\s+\")\n",
    "    user = re.compile(r\"(?i)@[a-z0-9_]+\")\n",
    "    text = whitespace.sub(' ', text)\n",
    "    text = user.sub('', text)\n",
    "    text = re.sub(r\"\\[[^()]*\\]\",\"\", text)\n",
    "    text = re.sub(\"\\d+\", \"\", text)\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = re.sub(r\"(?:@\\S*|#\\S*|http(?=.*://)\\S*)\", \"\", text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = [word for word in text.split() if word not in list(STOPWORDS)]\n",
    "\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>length_of_news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>million americans roll sleeves omicrontargeted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>american airlines flyer charged banned life pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COMEDY</td>\n",
       "      <td>funniest tweets cats dogs week sept dog dont u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PARENTING</td>\n",
       "      <td>funniest tweets parents week sept accidentally...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>woman called cops black birdwatcher loses laws...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    category                                     length_of_news\n",
       "0  U.S. NEWS  million americans roll sleeves omicrontargeted...\n",
       "1  U.S. NEWS  american airlines flyer charged banned life pu...\n",
       "2     COMEDY  funniest tweets cats dogs week sept dog dont u...\n",
       "3  PARENTING  funniest tweets parents week sept accidentally...\n",
       "4  U.S. NEWS  woman called cops black birdwatcher loses laws..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndf = final_df.copy()\n",
    "ndf.drop('len_news', inplace=True, axis=1)\n",
    "ndf['length_of_news'] = ndf['length_of_news'].apply(datacleaning)\n",
    "ndf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>length_of_news</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>million americans roll sleeves omicrontargeted...</td>\n",
       "      <td>[-1.3539314, 0.5398995, -1.3941106, 1.7469765,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>american airlines flyer charged banned life pu...</td>\n",
       "      <td>[-0.6059083, 0.037347153, -2.1189568, 0.673374...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COMEDY</td>\n",
       "      <td>funniest tweets cats dogs week sept dog dont u...</td>\n",
       "      <td>[0.56933457, 2.5814993, -3.8139722, -2.4902532...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PARENTING</td>\n",
       "      <td>funniest tweets parents week sept accidentally...</td>\n",
       "      <td>[-0.23393469, 0.6016026, -2.1117344, -0.011048...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>woman called cops black birdwatcher loses laws...</td>\n",
       "      <td>[-0.96607876, 0.16777916, -2.1811275, -0.02558...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    category                                     length_of_news  \\\n",
       "0  U.S. NEWS  million americans roll sleeves omicrontargeted...   \n",
       "1  U.S. NEWS  american airlines flyer charged banned life pu...   \n",
       "2     COMEDY  funniest tweets cats dogs week sept dog dont u...   \n",
       "3  PARENTING  funniest tweets parents week sept accidentally...   \n",
       "4  U.S. NEWS  woman called cops black birdwatcher loses laws...   \n",
       "\n",
       "                                              vector  \n",
       "0  [-1.3539314, 0.5398995, -1.3941106, 1.7469765,...  \n",
       "1  [-0.6059083, 0.037347153, -2.1189568, 0.673374...  \n",
       "2  [0.56933457, 2.5814993, -3.8139722, -2.4902532...  \n",
       "3  [-0.23393469, 0.6016026, -2.1117344, -0.011048...  \n",
       "4  [-0.96607876, 0.16777916, -2.1811275, -0.02558...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndf['vector'] = ndf['length_of_news'].apply(lambda text: en(text).vector)\n",
    "ndf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaydson/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chaydson/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "          ARTS       0.00      0.00      0.00       312\n",
      "ARTS & CULTURE       0.00      0.00      0.00       273\n",
      "  BLACK VOICES       0.00      0.00      0.00       894\n",
      "      BUSINESS       0.00      0.00      0.00      1125\n",
      "       COLLEGE       0.00      0.00      0.00       247\n",
      "        COMEDY       0.00      0.00      0.00      1128\n",
      "         CRIME       0.00      0.00      0.00       713\n",
      "CULTURE & ARTS       0.00      0.00      0.00       209\n",
      "       DIVORCE       0.00      0.00      0.00       670\n",
      "     EDUCATION       0.00      0.00      0.00       199\n",
      " ENTERTAINMENT       0.37      0.01      0.02      3438\n",
      "   ENVIRONMENT       0.00      0.00      0.00       299\n",
      "         FIFTY       0.00      0.00      0.00       271\n",
      "  FOOD & DRINK       0.45      0.00      0.01      1237\n",
      "     GOOD NEWS       0.00      0.00      0.00       290\n",
      "         GREEN       0.00      0.00      0.00       516\n",
      "HEALTHY LIVING       0.00      0.00      0.00      1317\n",
      " HOME & LIVING       0.00      0.00      0.00       871\n",
      "        IMPACT       0.00      0.00      0.00       683\n",
      " LATINO VOICES       0.00      0.00      0.00       238\n",
      "         MEDIA       0.00      0.00      0.00       575\n",
      "         MONEY       0.00      0.00      0.00       358\n",
      "     PARENTING       0.00      0.00      0.00      1755\n",
      "       PARENTS       0.00      0.00      0.00       774\n",
      "      POLITICS       0.17      1.00      0.29      7134\n",
      "  QUEER VOICES       0.00      0.00      0.00      1316\n",
      "      RELIGION       0.00      0.00      0.00       566\n",
      "       SCIENCE       0.00      0.00      0.00       475\n",
      "        SPORTS       0.00      0.00      0.00       986\n",
      "         STYLE       0.00      0.00      0.00       443\n",
      "STYLE & BEAUTY       0.00      0.00      0.00      1907\n",
      "         TASTE       0.00      0.00      0.00       404\n",
      "          TECH       0.00      0.00      0.00       409\n",
      " THE WORLDPOST       0.00      0.00      0.00       748\n",
      "        TRAVEL       0.00      0.00      0.00      2018\n",
      "     U.S. NEWS       0.00      0.00      0.00       268\n",
      "      WEDDINGS       0.00      0.00      0.00       756\n",
      "    WEIRD NEWS       0.00      0.00      0.00       544\n",
      "      WELLNESS       0.36      0.04      0.07      3644\n",
      "         WOMEN       0.00      0.00      0.00       705\n",
      "    WORLD NEWS       0.00      0.00      0.00       657\n",
      "     WORLDPOST       0.00      0.00      0.00       534\n",
      "\n",
      "      accuracy                           0.17     41906\n",
      "     macro avg       0.03      0.03      0.01     41906\n",
      "  weighted avg       0.10      0.17      0.06     41906\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaydson/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    ndf.vector.values,\n",
    "    ndf.category,\n",
    "    test_size=0.2,\n",
    "    random_state=2022\n",
    ")\n",
    "\n",
    "X_train_2d = np.stack(X_train)\n",
    "X_test_2d = np.stack(X_test)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_train_embed = scaler.fit_transform(X_train_2d)\n",
    "scaled_test_embed = scaler.transform(X_test_2d)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(scaled_train_embed, y_train)\n",
    "\n",
    "y_pred = clf.predict(scaled_test_embed)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "          ARTS       0.09      0.22      0.13       312\n",
      "ARTS & CULTURE       0.07      0.08      0.08       273\n",
      "  BLACK VOICES       0.15      0.20      0.17       894\n",
      "      BUSINESS       0.22      0.34      0.27      1125\n",
      "       COLLEGE       0.11      0.16      0.13       247\n",
      "        COMEDY       0.18      0.19      0.19      1128\n",
      "         CRIME       0.36      0.44      0.40       713\n",
      "CULTURE & ARTS       0.11      0.15      0.13       209\n",
      "       DIVORCE       0.28      0.41      0.34       670\n",
      "     EDUCATION       0.19      0.24      0.21       199\n",
      " ENTERTAINMENT       0.41      0.51      0.45      3438\n",
      "   ENVIRONMENT       0.22      0.23      0.23       299\n",
      "         FIFTY       0.05      0.06      0.06       271\n",
      "  FOOD & DRINK       0.53      0.62      0.57      1237\n",
      "     GOOD NEWS       0.17      0.05      0.08       290\n",
      "         GREEN       0.28      0.22      0.24       516\n",
      "HEALTHY LIVING       0.20      0.13      0.16      1317\n",
      " HOME & LIVING       0.60      0.46      0.52       871\n",
      "        IMPACT       0.18      0.13      0.15       683\n",
      " LATINO VOICES       0.16      0.03      0.04       238\n",
      "         MEDIA       0.30      0.15      0.20       575\n",
      "         MONEY       0.33      0.18      0.23       358\n",
      "     PARENTING       0.32      0.40      0.36      1755\n",
      "       PARENTS       0.19      0.07      0.11       774\n",
      "      POLITICS       0.58      0.73      0.65      7134\n",
      "  QUEER VOICES       0.55      0.32      0.40      1316\n",
      "      RELIGION       0.61      0.22      0.32       566\n",
      "       SCIENCE       0.66      0.26      0.37       475\n",
      "        SPORTS       0.61      0.36      0.45       986\n",
      "         STYLE       0.41      0.05      0.09       443\n",
      "STYLE & BEAUTY       0.48      0.66      0.56      1907\n",
      "         TASTE       0.20      0.06      0.09       404\n",
      "          TECH       0.46      0.17      0.25       409\n",
      " THE WORLDPOST       0.44      0.26      0.33       748\n",
      "        TRAVEL       0.57      0.58      0.57      2018\n",
      "     U.S. NEWS       0.18      0.07      0.11       268\n",
      "      WEDDINGS       0.61      0.38      0.47       756\n",
      "    WEIRD NEWS       0.38      0.05      0.08       544\n",
      "      WELLNESS       0.50      0.55      0.52      3644\n",
      "         WOMEN       0.31      0.09      0.14       705\n",
      "    WORLD NEWS       0.30      0.14      0.19       657\n",
      "     WORLDPOST       0.16      0.18      0.17       534\n",
      "\n",
      "      accuracy                           0.42     41906\n",
      "     macro avg       0.33      0.26      0.27     41906\n",
      "  weighted avg       0.42      0.42      0.40     41906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors = 5, metric= 'euclidean')\n",
    "clf.fit(X_train_2d, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_2d)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input data:  (209527,)\n",
      "shape of target variable:  (209527,)\n",
      "Length of word index: 202192\n"
     ]
    }
   ],
   "source": [
    "X = ndf['length_of_news']\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(ndf['category'])\n",
    "print(\"shape of input data: \", X.shape)\n",
    "print(\"shape of target variable: \", y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100000, oov_token='<00V>') \n",
    "tokenizer.fit_on_texts(X_train) # build the word index\n",
    "# padding X_train text input data\n",
    "train_seq = tokenizer.texts_to_sequences(X_train) # converts strinfs into integer lists\n",
    "train_padseq = pad_sequences(train_seq, maxlen=130) # pads the integer lists to 2D integer tensor \n",
    "\n",
    "# padding X_test text input data\n",
    "test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "test_padseq = pad_sequences(test_seq, maxlen=130)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "max_words = 150000  # total number of words to consider in embedding layer\n",
    "total_words = len(word_index)\n",
    "maxlen = 130 # max length of sequence \n",
    "y_train = to_categorical(y_train, num_classes=42)\n",
    "y_test = to_categorical(y_test, num_classes=42)\n",
    "print(\"Length of word index:\", total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 130, 100)          20219200  \n",
      "                                                                 \n",
      " bidirectional_9 (Bidirecti  (None, 130, 128)          84480     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_10 (Bidirect  (None, 130, 128)          98816     \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_11 (Bidirect  (None, 130, 128)          24704     \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 128, 72)           27720     \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPoolin  (None, 64, 72)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " simple_rnn_7 (SimpleRNN)    (None, 64, 64)            8768      \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (None, 64)                24960     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 42)                2730      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20491378 (78.17 MB)\n",
      "Trainable params: 20491378 (78.17 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(total_words, 100, input_length=maxlen))\n",
    "model2.add(Bidirectional(LSTM(64, dropout=0.1, recurrent_dropout=0.10, activation='tanh', return_sequences=True)))\n",
    "model2.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model2.add(Bidirectional(SimpleRNN(64, dropout=0.2, recurrent_dropout=0.20, activation='tanh', return_sequences=True)))\n",
    "model2.add(Conv1D(72, 3, activation='relu'))\n",
    "model2.add(MaxPooling1D(2))\n",
    "model2.add(SimpleRNN(64, activation='tanh', dropout=0.2, recurrent_dropout=0.20, return_sequences=True))\n",
    "model2.add(GRU(64, recurrent_dropout=0.20, recurrent_regularizer='l1_l2'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(42, activation='softmax'))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1048/1048 [==============================] - 873s 809ms/step - loss: 3.0211 - accuracy: 0.3089 - val_loss: 2.3715 - val_accuracy: 0.3917\n",
      "Epoch 2/15\n",
      "1048/1048 [==============================] - 553s 528ms/step - loss: 2.1124 - accuracy: 0.4628 - val_loss: 1.9881 - val_accuracy: 0.4957\n",
      "Epoch 3/15\n",
      "1048/1048 [==============================] - 555s 530ms/step - loss: 1.8042 - accuracy: 0.5389 - val_loss: 1.9132 - val_accuracy: 0.5144\n",
      "Epoch 4/15\n",
      "1048/1048 [==============================] - 554s 529ms/step - loss: 1.6151 - accuracy: 0.5845 - val_loss: 1.8548 - val_accuracy: 0.5317\n",
      "Epoch 5/15\n",
      "1048/1048 [==============================] - 554s 528ms/step - loss: 1.4684 - accuracy: 0.6238 - val_loss: 1.8162 - val_accuracy: 0.5460\n",
      "Epoch 6/15\n",
      "1048/1048 [==============================] - 554s 529ms/step - loss: 1.3422 - accuracy: 0.6545 - val_loss: 1.8426 - val_accuracy: 0.5497\n",
      "Epoch 7/15\n",
      "1048/1048 [==============================] - 552s 527ms/step - loss: 1.2329 - accuracy: 0.6832 - val_loss: 1.8564 - val_accuracy: 0.5538\n",
      "Epoch 8/15\n",
      "1048/1048 [==============================] - 554s 528ms/step - loss: 1.1357 - accuracy: 0.7113 - val_loss: 1.8984 - val_accuracy: 0.5460\n",
      "Epoch 9/15\n",
      "1048/1048 [==============================] - 556s 531ms/step - loss: 1.0385 - accuracy: 0.7388 - val_loss: 1.9974 - val_accuracy: 0.5361\n",
      "Epoch 10/15\n",
      "1048/1048 [==============================] - 568s 542ms/step - loss: 0.9479 - accuracy: 0.7648 - val_loss: 2.0325 - val_accuracy: 0.5325\n",
      "Epoch 11/15\n",
      "1048/1048 [==============================] - 567s 541ms/step - loss: 0.8621 - accuracy: 0.7903 - val_loss: 2.1158 - val_accuracy: 0.5305\n",
      "Epoch 12/15\n",
      "1048/1048 [==============================] - 564s 539ms/step - loss: 0.7808 - accuracy: 0.8140 - val_loss: 2.1615 - val_accuracy: 0.5308\n",
      "Epoch 13/15\n",
      "1048/1048 [==============================] - 562s 537ms/step - loss: 0.7077 - accuracy: 0.8342 - val_loss: 2.2358 - val_accuracy: 0.5247\n",
      "Epoch 14/15\n",
      "1048/1048 [==============================] - 561s 535ms/step - loss: 0.6398 - accuracy: 0.8524 - val_loss: 2.3028 - val_accuracy: 0.5257\n",
      "Epoch 15/15\n",
      "1048/1048 [==============================] - 578s 551ms/step - loss: 0.5866 - accuracy: 0.8680 - val_loss: 2.3791 - val_accuracy: 0.5173\n",
      "test loss and accuracy: 2.3606834411621094 0.5219538807868958\n"
     ]
    }
   ],
   "source": [
    "model2.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "# SETUP A EARLY STOPPING CALL and model check point API\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='accuracy',\n",
    "                                              patience=5,\n",
    "                                              verbose=1,\n",
    "                                              mode='min'\n",
    "                                              )\n",
    "checkpointer = ModelCheckpoint(filepath='bestvalue1',moniter='val_loss', verbose=0, save_best_only=True)\n",
    "callback_list = [checkpointer, earlystopping]\n",
    "\n",
    "# fit model to the data\n",
    "history2 = model2.fit(train_padseq, y_train, \n",
    "                     batch_size=128, \n",
    "                     epochs=15, \n",
    "                     validation_split=0.2,\n",
    "                     shuffle=True\n",
    "                    )\n",
    "\n",
    "# evalute the model\n",
    "test_loss2, test_acc2 = model2.evaluate(test_padseq, y_test, verbose=0)\n",
    "print(\"test loss and accuracy:\", test_loss2, test_acc2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
