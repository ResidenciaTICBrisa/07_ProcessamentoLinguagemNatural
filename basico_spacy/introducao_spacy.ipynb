{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:2.8rem;color:#2596be;\">Introdução ao spaCy</h1>\n",
    "\n",
    "O spaCy é uma biblioteca gratuita e _open source_ para **Processamento de Linguagem Natural** avançado em Python.\n",
    "\n",
    "Foi especificamente projetada para uso em produção e para auxiliar na criação de aplicações que processam e \"entendem\" grandes volumes de texto.\n",
    "\n",
    "Ela pode ser usada para realizar a extração de informações, construir sistemas de compreensão de linguagem natural ou para pré-processar texto para modelos de _deep learning_."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capacidades\n",
    "\n",
    "### Features\n",
    "\n",
    "Todos os recursos e capacidades da biblioteca se situam em conceitos linguísticos e funcionalidades gerais de _machine learning_. Podemos citar:\n",
    "\n",
    "| Name |\tDescription |\n",
    "| :- | :- |\n",
    "| Tokenização |\tSegmentação do texto em sentenças, palavras, pontuações, etc. |\n",
    "| Classificação gramatical (_Part-of-Speech Tagging_ ou _POS Tagging_) | Atribuição de tipos de palavras aos tokens (substantivo, adjetivo, verbo, etc.). |\n",
    "| Análise de dependência | Atribuição de _labels_ de dependência sintática, que descrevem a relação entre tokens individuais, como sujeito ou objeto. |\n",
    "| Lematização | Atribuição das formas básicas das palavras (viveriam-viver, carros-carro, ...). |\n",
    "| Detecção de limite de frase (_Sentence Boundary Detection_ ou SBD) | Identificação e segmentação de frases/sentenças individuais. |\n",
    "| Reconhecimento de entidades nomeadas (_Named Entity Recognition_ ou NER) | Rotulação de objetos do \"mundo real\", como pessoas, empresas ou lugares. |\n",
    "| Vinculação de entidades (_Entity Linking_ ou EL) | Remoção de ambiguidade em entidades textuais, criando identificadores únicos para cada uma em uma base de conhecimento. |\n",
    "| Similaridade | Comparação de palavras, intervalos de textos (_spans_) e documentos, e aferição (quão similar esses elementos são com cada um). |\n",
    "| Classificação textual | Atribuição de categorias ou _labels_  para todo um documento, ou partes dele. |\n",
    "| Correspondência baseada em regras | Busca de sequência de tokens baseada em anotações textuais ou linguísticas (<strike>Regex com esteróides</strike>) |\n",
    "| Treinamento | Atualização e aperfeiçoamento de modelos estatísticos de predição |\n",
    "| Serialização | Guardando objetos e arquivos ou _byte strings_ |\n",
    "\n",
    "Mais detalhes podem ser encontrados no tópico de [anotações linguísticas](https://spacy.io/usage/spacy-101#annotations) e em outros tópicos na documentação.\n",
    "\n",
    "### Pipeline\n",
    "\n",
    "Alguns recursos do spaCy podem funcionar de maneira independente, enquanto outros necessitam ser carregados de modelos de pipelines treinados. De toda forma, a estrutura e funcionamento da biblioteca se dá pela definição e utilização de um pipeline.\n",
    "\n",
    "Um pipeline treinado consiste em vários componentes que usam um modelo estatístico (_machine learning_) treinado com dados rotulados. O spaCy atualmente oferece diversos pipelines treinados para uma variedade de linguagens (incluindo o português), que podem ser instaladas como módulos Python individuais.\n",
    "\n",
    "![Pipeline](https://spacy.io/images/pipeline.svg)\n",
    "\n",
    "Ao chamar o objeto `nlp` para um texto, o spaCy primeiro tokeniza o texto para produzir um objeto `Doc`. O `Doc` é então processado em várias etapas diferentes, com base nos componentes definidos do pipeline. Essas etapas são referidas como **pipeline de processamento**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalação\n",
    "\n",
    "A instalação pode ser feita com uma série de configurações, com base no sistema operacional, plataforma, gerenciador de pacotes, hardware, etc. Nesta introdução, será abordada apenas uma forma através do `pip`, para Linux, com procesamento em CPU, plataforma x86.\n",
    "\n",
    "Mais detalhes no tópico de [instalação](https://spacy.io/usage) da documentação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Criação e utilização de ambiente virtual, caso queira instalar localmente\n",
    "\n",
    "python -m venv .env   #Criando\n",
    "source .env/bin/activate    #Ativando\n",
    "# deactivate    #Desativando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install -U pip setuptools wheel\n",
    "pip install -U spacy pandas\n",
    "python -m spacy download pt_core_news_sm    #Modelo de pipeline em pt para eficiência\n",
    "# python -m spacy download pt_core_news_lg    #Modelo de pipeline em pt para precisão"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Básico\n",
    "\n",
    "É possível carregar um pipeline vazio, habilitar componentes nele ou então carregar um pipeline completo e desabilitar componentes. Porém, isso deve ser feito com cuidado pois alguns componentes podem depender de outro.\n",
    "\n",
    "Geralmente, um pipeline vazio é utilizado quando não existe um pipeline pré-treinado para a linguagem, para treinar o próprio modelo ou então para uso somente da tokenização e de regras específicas da linguagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando um objeto nlp vazio da lingua portuguesa\n",
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.blank (\"pt\")\n",
    "nlp.pipe_names  #Retorna uma lista com os nomes dos componentes do pipeline (no caso, uma lista vazia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'morphologizer', 'parser', 'lemmatizer', 'attribute_ruler', 'ner']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carregando um pipeline pré-treinado (através do módulo do spaCy ou pelo módulo do modelo)\n",
    "import spacy\n",
    "# import pt_core_news_sm as pt_model\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "# nlp = pt_model.load()\n",
    "nlp.pipe_names\n",
    "\n",
    "# nlp.disable_pipe(\"tok2vec\")   #Para desabilitar componente\n",
    "# nlp.enable_pipe(\"tok2vec\")    #Para habilitar componente"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando você processa um texto com um objeto `nlp`, o spaCy cria um objeto `Doc` (abreviação de \"documento\"). Nele, é possível acessar várias informações do texto de forma estruturada, sem perder nenhuma informação original.\n",
    "\n",
    "O `Doc` se comporta como uma sequência do Python, permitindo iterações nos tokens e acesso pelo índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto: Apresento-lhes  Lemma: apresento-lhe   Pos: VERB        \n",
      "Texto: a               Lemma: a               Pos: ADP         \n",
      "Texto: frase-exemplo   Lemma: frase-exemplo   Pos: NOUN        \n",
      "Texto: .               Lemma: .               Pos: PUNCT       \n",
      "Texto: Esta            Lemma: este            Pos: PRON        \n",
      "Texto: é               Lemma: ser             Pos: AUX         \n",
      "Texto: uma             Lemma: um              Pos: DET         \n",
      "Texto: sra.            Lemma: sra.            Pos: NOUN        \n",
      "Texto: frase           Lemma: frase           Pos: NOUN        \n",
      "Texto: como            Lemma: como            Pos: ADP         \n",
      "Texto: exemplo         Lemma: exemplo         Pos: NOUN        \n",
      "Texto: para            Lemma: para            Pos: ADP         \n",
      "Texto: vocês           Lemma: você            Pos: PRON        \n",
      "Texto: .               Lemma: .               Pos: PUNCT       \n"
     ]
    }
   ],
   "source": [
    "# Processando um texto como exemplo básico\n",
    "doc = nlp(\"Apresento-lhes a frase-exemplo. Esta é uma sra. frase como exemplo para vocês.\")\n",
    "print('\\n'.join([f'Texto: {w.text:<16}Lemma: {w.lemma_:<16}Pos: {w.pos_:<12}' for w in doc]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Com propostas do Brasil Participativo\n",
    "\n",
    "No caso do nosso projeto, iremos processar propostas que foram submetidas na plataforma do Brasil Participativo. Idealmente e em um primeiro momento, necessitamos apenas processar os dados contidos nos campos de título e descrição das propostas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TÍTULO: Centro de tratamento para pneumologia \n",
      "\n",
      "Texto: Centro         Lemma: centro         Pos: NOUN      \n",
      "Texto: de             Lemma: de             Pos: ADP       \n",
      "Texto: tratamento     Lemma: tratamento     Pos: NOUN      \n",
      "Texto: para           Lemma: para           Pos: ADP       \n",
      "Texto: pneumologia    Lemma: pneumologia    Pos: NOUN      \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "DESCRIÇÃO: A pneumologia está necessitando com urgência de um centro de tratamento para pacientes com HAP, Broctassia cística, HAS, fibrose cística, ect .\n",
      "Nosso estado por ser um Polo de saúde está deixando a deseja.\n",
      "Urgência pessoa estão morendo a volta do diagnóstico.\n",
      "\n",
      "Texto: A              Lemma: o              Pos: DET       \n",
      "Texto: pneumologia    Lemma: pneumologia    Pos: NOUN      \n",
      "Texto: está           Lemma: estar          Pos: AUX       \n",
      "Texto: necessitando   Lemma: necessitar     Pos: VERB      \n",
      "Texto: com            Lemma: com            Pos: ADP       \n",
      "Texto: urgência       Lemma: urgência       Pos: NOUN      \n",
      "Texto: de             Lemma: de             Pos: ADP       \n",
      "Texto: um             Lemma: um             Pos: DET       \n",
      "Texto: centro         Lemma: centro         Pos: NOUN      \n",
      "Texto: de             Lemma: de             Pos: ADP       \n",
      "Texto: tratamento     Lemma: tratamento     Pos: NOUN      \n",
      "Texto: para           Lemma: para           Pos: ADP       \n",
      "Texto: pacientes      Lemma: paciente       Pos: NOUN      \n",
      "Texto: com            Lemma: com            Pos: ADP       \n",
      "Texto: HAP            Lemma: HAP            Pos: PROPN     \n",
      "Texto: ,              Lemma: ,              Pos: PUNCT     \n",
      "Texto: Broctassia     Lemma: Broctassia     Pos: PROPN     \n",
      "Texto: cística        Lemma: cístico        Pos: PROPN     \n",
      "Texto: ,              Lemma: ,              Pos: PUNCT     \n",
      "Texto: HAS            Lemma: HAS            Pos: PROPN     \n",
      "Texto: ,              Lemma: ,              Pos: PUNCT     \n",
      "Texto: fibrose        Lemma: fibrose        Pos: NOUN      \n",
      "Texto: cística        Lemma: cístico        Pos: ADJ       \n",
      "Texto: ,              Lemma: ,              Pos: PUNCT     \n",
      "Texto: ect            Lemma: ect            Pos: PROPN     \n",
      "Texto: .              Lemma: .              Pos: PUNCT     \n",
      "Texto: \n",
      "              Lemma: \n",
      "              Pos: SPACE     \n",
      "Texto: Nosso          Lemma: nosso          Pos: DET       \n",
      "Texto: estado         Lemma: estado         Pos: NOUN      \n",
      "Texto: por            Lemma: por            Pos: SCONJ     \n",
      "Texto: ser            Lemma: ser            Pos: AUX       \n",
      "Texto: um             Lemma: um             Pos: DET       \n",
      "Texto: Polo           Lemma: Polo           Pos: PROPN     \n",
      "Texto: de             Lemma: de             Pos: ADP       \n",
      "Texto: saúde          Lemma: saúde          Pos: NOUN      \n",
      "Texto: está           Lemma: estar          Pos: AUX       \n",
      "Texto: deixando       Lemma: deixar         Pos: VERB      \n",
      "Texto: a              Lemma: o              Pos: DET       \n",
      "Texto: deseja         Lemma: deseja         Pos: NOUN      \n",
      "Texto: .              Lemma: .              Pos: PUNCT     \n",
      "Texto: \n",
      "              Lemma: \n",
      "              Pos: SPACE     \n",
      "Texto: Urgência       Lemma: Urgência       Pos: NOUN      \n",
      "Texto: pessoa         Lemma: pessoa         Pos: NOUN      \n",
      "Texto: estão          Lemma: estar          Pos: AUX       \n",
      "Texto: morendo        Lemma: morer          Pos: VERB      \n",
      "Texto: a              Lemma: o              Pos: DET       \n",
      "Texto: volta          Lemma: volta          Pos: NOUN      \n",
      "Texto: do             Lemma: de o           Pos: ADP       \n",
      "Texto: diagnóstico    Lemma: diagnóstico    Pos: NOUN      \n",
      "Texto: .              Lemma: .              Pos: PUNCT     \n"
     ]
    }
   ],
   "source": [
    "# Processando um proposta como exemplo\n",
    "import pandas as pd\n",
    "\n",
    "propostas = pd.read_csv('./propostas.csv')\n",
    "docTitle = nlp(propostas['Título'].iloc[0])\n",
    "docDesc = nlp(propostas['Descrição'].iloc[0])\n",
    "\n",
    "print(f'TÍTULO: {docTitle}\\n')\n",
    "print('\\n'.join([f'Texto: {w.text:<15}Lemma: {w.lemma_:<15}Pos: {w.pos_:<10}' for w in docTitle]))\n",
    "print('\\n---------------------------------------------------------------------------------\\n')\n",
    "print(f'DESCRIÇÃO: {docDesc}\\n')\n",
    "print('\\n'.join([f'Texto: {w.text:<15}Lemma: {w.lemma_:<15}Pos: {w.pos_:<10}' for w in docDesc]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Perceba que o modelo de pipeline pré-treinado não acerta 100% dos casos, pois se trata de modelos de _machine learning_. Pode ser necessário a modificação (um ajuste fino) das métricas de algum componente para melhorar sua precisão ou mesmo o treinamento de novos componentes para substituir os utilizados."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências e Conteúdos Complementares\n",
    "\n",
    "- [Spacy 101](https://spacy.io/usage/spacy-101)\n",
    "- [PLN avançado com spaCy - Curso completo](https://course.spacy.io/pt)\n",
    "- [Pipelines em PT](https://spacy.io/models/pt)\n",
    "- Aba de **Guides** da [Documentação oficial](https://spacy.io/usage)\n",
    "\n",
    "Obs.: Recomendo começar pelo capítulo 1 do curso de PLN avançado."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
